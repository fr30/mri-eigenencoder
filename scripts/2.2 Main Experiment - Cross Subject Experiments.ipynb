{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0569ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "\n",
    "# SLICE = 1000\n",
    "\n",
    "# eeg_data = np.load('./run_experiments_ssl_signal/eeg_data.npy', mmap_mode='r')\n",
    "# emg_data = np.load('./run_experiments_ssl_signal/emg_data.npy', mmap_mode='r')\n",
    "# subject = np.load('./run_experiments_ssl_signal/subject.npy')\n",
    "# label = np.load('./run_experiments_ssl_signal/label.npy')\n",
    "# new_label = np.load('./run_experiments_ssl_signal/new_label.npy')\n",
    "\n",
    "# idx = np.random.choice(len(eeg_data), SLICE)\n",
    "# eeg_data = eeg_data[idx]\n",
    "# emg_data = emg_data[idx]\n",
    "# subject = subject[idx]\n",
    "# label = label[idx]\n",
    "# new_label = new_label[idx]\n",
    "\n",
    "# np.save('./run_experiments_ssl_signal/eeg_data_sample.npy', eeg_data)\n",
    "# np.save('./run_experiments_ssl_signal/emg_data_sample.npy', emg_data)\n",
    "# np.save('./run_experiments_ssl_signal/subject_sample.npy', subject)\n",
    "# np.save('./run_experiments_ssl_signal/label_sample.npy', label)\n",
    "# np.save('./run_experiments_ssl_signal/new_label_sample.npy', new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e489684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 4000])\n",
      "torch.Size([1, 7, 4000])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "subject = np.load('./run_experiments_ssl_signal/subject_sample.npy')\n",
    "label = np.load('./run_experiments_ssl_signal/label_sample.npy')\n",
    "new_label = np.load('./run_experiments_ssl_signal/new_label_sample.npy')\n",
    "eeg_data = np.load('./run_experiments_ssl_signal/eeg_data_sample.npy')\n",
    "emg_data = np.load('./run_experiments_ssl_signal/emg_data_sample.npy')\n",
    "\n",
    "subject = torch.from_numpy(subject).long()\n",
    "label = torch.from_numpy(label).long()\n",
    "new_label = torch.from_numpy(new_label).long()\n",
    "eeg_data = torch.from_numpy(eeg_data).float()\n",
    "emg_data = torch.from_numpy(emg_data).float()\n",
    "\n",
    "SAMPLE_X = eeg_data\n",
    "SAMPLE_Y = emg_data\n",
    "label_tensor = new_label\n",
    "\n",
    "print(SAMPLE_X[:1].shape)\n",
    "print(SAMPLE_Y[:1].shape)\n",
    "print(label_tensor[:1].shape)\n",
    "# Generate Data Cross Subject\n",
    "\n",
    "train_idx = [np.where(subject == k)[0] for k in range(0, 20)]\n",
    "test_idx = [np.where(subject == k)[0] for k in range(20, 25)]\n",
    "\n",
    "train_idx = np.concatenate(train_idx)\n",
    "test_idx = np.concatenate(test_idx)\n",
    "\n",
    "train_major_label, test_major_label = new_label[train_idx], new_label[test_idx]\n",
    "train_subtle_label, test_subtle_label = label[train_idx], label[test_idx]\n",
    "train_subj_label, test_subj_label = subject[train_idx], subject[test_idx]\n",
    "\n",
    "train_X, test_X = SAMPLE_X[train_idx], SAMPLE_X[test_idx]\n",
    "train_Y, test_Y = SAMPLE_Y[train_idx], SAMPLE_Y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f601322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Subset, Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "from sklearn import svm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, dim_features=128, num_classes = 10):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_features, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)  # No activation, CrossEntropyLoss includes softmax\n",
    "        return x\n",
    "    \n",
    "class NETWORK_F_MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 200, out_dim = 200, how_many_layers = 2):\n",
    "        super(NETWORK_F_MLP, self).__init__()\n",
    "        self.dim = out_dim\n",
    "        self.many_layer = how_many_layers\n",
    "        \n",
    "        self.fc_list = []\n",
    "        self.bn_list = []\n",
    "        \n",
    "#         self.fc_list.append(nn.Linear(input_dim+20, HIDDEN, bias=True))\n",
    "        self.fc_list.append(nn.Linear(input_dim, HIDDEN, bias=True))\n",
    "        self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "\n",
    "        for i in range(0, self.many_layer-1):\n",
    "            self.fc_list.append(nn.Linear(HIDDEN, HIDDEN, bias=True))\n",
    "            self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "            \n",
    "        self.fc_list = nn.ModuleList(self.fc_list)\n",
    "        self.bn_list = nn.ModuleList(self.bn_list)\n",
    "\n",
    "        self.fc_final = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        for i in range(0, self.many_layer):\n",
    "            x = self.fc_list[i](x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.bn_list[i](x)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Advanced1DCNN_channel(nn.Module):\n",
    "    def __init__(self, input_channel=1, num_classes=100, input_size=4000):\n",
    "        super(Advanced1DCNN_channel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channel, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(int(int(int(int(int(input_size/4)/4)/4)/4))*256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.MLP = NETWORK_F_MLP(input_dim = 128, HIDDEN = 2000, out_dim = num_classes, how_many_layers = 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, channel = x.shape[0], x.shape[1]\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "#         out = torch.sigmoid(out)\n",
    "        out = self.MLP(out)\n",
    "        return out\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        # If the folder does not exist, create it\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "\n",
    "# net = Advanced1DCNN_channel(60, 128, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099216d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './RESULTS_CROSS_SUBJECTS/' already exists.\n",
      "1271956.875 744974.1875\n",
      "1666906.75 1791732.625\n",
      "2756906.75 5862288.0\n",
      "4074855.5 12677123.0\n",
      "7429062.5 59093812.0\n",
      "7199569.5 29227276.0\n",
      "9294102.0 43846408.0\n",
      "8998592.0 79541280.0\n",
      "8650842.0 123732208.0\n",
      "12855560.0 130775544.0\n",
      "9361105.0 77195336.0\n",
      "10495752.0 59360344.0\n",
      "13109358.0 190539696.0\n",
      "16911172.0 152007344.0\n",
      "15496298.0 178330032.0\n",
      "15414418.0 211076048.0\n",
      "19144766.0 95994752.0\n",
      "14582872.0 149261616.0\n",
      "17118486.0 100459144.0\n",
      "21030280.0 275908640.0\n",
      "18166410.0 363794560.0\n",
      "22837336.0 397053664.0\n",
      "25342320.0 125903768.0\n",
      "20607782.0 59343480.0\n",
      "23798346.0 150632096.0\n",
      "22411536.0 154007408.0\n",
      "24998378.0 239474384.0\n",
      "24276828.0 129506416.0\n",
      "24513048.0 253057344.0\n",
      "24397806.0 211464432.0\n",
      "24719596.0 429449952.0\n",
      "25908254.0 140481232.0\n",
      "23581842.0 142174592.0\n",
      "25021272.0 103208944.0\n",
      "23113356.0 396699744.0\n",
      "26730130.0 151634368.0\n",
      "28465012.0 127137496.0\n",
      "24126472.0 233139712.0\n",
      "27954452.0 136505328.0\n",
      "26128172.0 121925176.0\n",
      "27383120.0 314204064.0\n",
      "19803442.0 97563968.0\n",
      "25519576.0 283417184.0\n",
      "27382692.0 277211808.0\n",
      "23227700.0 140567472.0\n",
      "29096608.0 90273728.0\n",
      "28701762.0 505352704.0\n",
      "25360220.0 279004992.0\n",
      "25805296.0 104523304.0\n",
      "32590924.0 354756544.0\n",
      "29699410.0 207364192.0\n",
      "27415822.0 287355360.0\n",
      "25100448.0 131040024.0\n",
      "27090590.0 243995056.0\n",
      "29398060.0 217907296.0\n",
      "29030260.0 285248832.0\n",
      "30867028.0 352352608.0\n",
      "29214080.0 2720857344.0\n",
      "29024414.0 327635744.0\n",
      "29401220.0 293119840.0\n",
      "27880784.0 423712480.0\n",
      "29868120.0 333465856.0\n",
      "31172246.0 735864448.0\n",
      "33217836.0 123042408.0\n",
      "31807544.0 558867392.0\n",
      "30863046.0 1072096576.0\n",
      "31808306.0 572183808.0\n",
      "31046088.0 371043680.0\n",
      "30794328.0 312303296.0\n",
      "33467212.0 562404544.0\n",
      "31643428.0 167032192.0\n",
      "31582088.0 819396864.0\n",
      "32787884.0 437927712.0\n",
      "30430906.0 176046448.0\n",
      "32459028.0 185672096.0\n",
      "31353492.0 1650366464.0\n",
      "30928364.0 768985984.0\n",
      "36113076.0 382850400.0\n",
      "31757992.0 191764192.0\n",
      "34064684.0 1458009216.0\n",
      "31949238.0 526300256.0\n",
      "36746188.0 537065216.0\n",
      "34699676.0 213135024.0\n",
      "32172878.0 227037328.0\n",
      "34014816.0 322306048.0\n",
      "32266752.0 156186896.0\n",
      "38752512.0 1210792704.0\n",
      "30645972.0 123034632.0\n",
      "34545420.0 459441344.0\n",
      "35208136.0 320753664.0\n",
      "33712228.0 200046832.0\n",
      "35198196.0 1065076608.0\n",
      "37327796.0 377438720.0\n",
      "33513296.0 463792352.0\n",
      "34317452.0 704740352.0\n",
      "34671532.0 588575808.0\n",
      "37791992.0 468314080.0\n",
      "38093952.0 573356608.0\n",
      "34267308.0 226442640.0\n",
      "36468440.0 319100928.0\n",
      "Step 100, loss -0.11428497731685638\n",
      "35311672.0 672579008.0\n",
      "38066712.0 177822656.0\n",
      "38525696.0 3480318976.0\n",
      "40145648.0 484921440.0\n",
      "38952728.0 307203616.0\n",
      "40307036.0 576851712.0\n",
      "37744568.0 1030157824.0\n",
      "38676164.0 317351008.0\n",
      "38591564.0 410960096.0\n",
      "38685152.0 158431248.0\n",
      "37883092.0 334246720.0\n",
      "39480572.0 309524128.0\n",
      "42087400.0 746019200.0\n",
      "41561348.0 851677440.0\n",
      "39157076.0 472735104.0\n",
      "40785024.0 789104384.0\n",
      "39500600.0 422858048.0\n",
      "40880948.0 500001728.0\n",
      "40580192.0 607153600.0\n",
      "43051492.0 297196288.0\n",
      "40889896.0 1656903552.0\n",
      "42362012.0 361115552.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m input_x \u001b[38;5;241m=\u001b[39m train_X[batch_indices]\n\u001b[1;32m    243\u001b[0m input_y \u001b[38;5;241m=\u001b[39m train_Y[batch_indices]\n\u001b[0;32m--> 245\u001b[0m feature_1 \u001b[38;5;241m=\u001b[39m NET_1(\u001b[43minput_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    246\u001b[0m feature_2 \u001b[38;5;241m=\u001b[39m NET_2(input_y\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m    248\u001b[0m RF \u001b[38;5;241m=\u001b[39m (feature_1\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@feature_1\u001b[39m)\u001b[38;5;241m/\u001b[39mfeature_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "class NETWORK_F_MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 200, out_dim = 200, how_many_layers = 2):\n",
    "        super(NETWORK_F_MLP, self).__init__()\n",
    "        self.dim = out_dim\n",
    "        self.many_layer = how_many_layers\n",
    "        \n",
    "        self.fc_list = []\n",
    "        self.bn_list = []\n",
    "        \n",
    "#         self.fc_list.append(nn.Linear(input_dim+20, HIDDEN, bias=True))\n",
    "        self.fc_list.append(nn.Linear(input_dim, HIDDEN, bias=True))\n",
    "        self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "\n",
    "        for i in range(0, self.many_layer-1):\n",
    "            self.fc_list.append(nn.Linear(HIDDEN, HIDDEN, bias=True))\n",
    "            self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "            \n",
    "        self.fc_list = nn.ModuleList(self.fc_list)\n",
    "        self.bn_list = nn.ModuleList(self.bn_list)\n",
    "\n",
    "        self.fc_final = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        for i in range(0, self.many_layer):\n",
    "            x = self.fc_list[i](x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.bn_list[i](x)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Advanced1DCNN_channel(nn.Module):\n",
    "    def __init__(self, input_channel=1, num_classes=100, input_size=4000, num_channel=60):\n",
    "        super(Advanced1DCNN_channel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 15, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.MLP = NETWORK_F_MLP(input_dim = 128*input_channel, HIDDEN = 4000, out_dim = num_classes, how_many_layers = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, channel = x.shape[0], x.shape[1]\n",
    "        x = x.unsqueeze(2)\n",
    "        x = x.flatten(0, 1)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = out.reshape(bs, channel, -1)\n",
    "        out = out.flatten(-2, -1)\n",
    "        out = self.MLP(out)\n",
    "        return out\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, dim_features=128, num_classes = 10):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(dim_features, 256)\n",
    "#         self.bn1 = nn.BatchNorm1d(256)\n",
    "#         self.fc2 = nn.Linear(256, 512)\n",
    "#         self.bn2 = nn.BatchNorm1d(512)\n",
    "#         self.fc3 = nn.Linear(512, 256)\n",
    "#         self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(dim_features, num_classes)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.relu(self.bn1(self.fc1(x)))\n",
    "#         x = torch.relu(self.bn2(self.fc2(x)))\n",
    "#         x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)  # No activation, CrossEntropyLoss includes softmax\n",
    "        # Using a linear network for channel network  \n",
    "        return x\n",
    "\n",
    "step = 0\n",
    "def fmcat_loss(RF, RG, P):\n",
    "    global step\n",
    "    eps = torch.eye(RF.shape[1], device=RF.device) * 1e-6\n",
    "    \n",
    "    lhs = torch.linalg.solve(RF + eps, P + eps)\n",
    "    rhs = torch.linalg.solve(RG + eps, P.T + eps)\n",
    "    tsd_m = lhs @ rhs\n",
    "    tsd = -torch.trace(tsd_m / torch.norm(tsd_m).detach())\n",
    "\n",
    "    # lhsn = lhs / torch.norm(lhs).detach()\n",
    "    # rhsn = rhs / torch.norm(rhs).detach()\n",
    "    # tsd = -torch.trace(lhsn @ rhsn)\n",
    "\n",
    "    # if step == 0:\n",
    "    #     # print(RF)\n",
    "    #     # print(lhs)\n",
    "    #     print(rhs)\n",
    "    #     print(torch.linalg.solve(RG, P))\n",
    "    #     print(RG)\n",
    "    #     print(P.T)\n",
    "\n",
    "    step += 1\n",
    "    # eye = torch.eye(lhs.shape[0]).to(lhs.device)\n",
    "    # MF = fmri_f.T @ fmri_f\n",
    "    # MG = smri_f.T @ smri_f\n",
    "    # reg = torch.square(MF.T @ MF - eye).sum() + torch.square(MG.T @ MG - eye).sum()\n",
    "\n",
    "    return tsd\n",
    "\n",
    "def return_cost_trace(RFG, track_cov_estimate_final):\n",
    "    RF_E = track_cov_estimate_final[:128, :128]\n",
    "    RG_E = track_cov_estimate_final[128:, 128:]\n",
    "    P_E = track_cov_estimate_final[:128, 128:]\n",
    "\n",
    "    RF_EI = torch.inverse(RF_E)\n",
    "    RG_EI = torch.inverse(RG_E)\n",
    "\n",
    "    RF = RFG[:128, :128]\n",
    "    RG = RFG[128:, 128:]\n",
    "    P = RFG[:128, 128:]\n",
    "\n",
    "    COST = -RF_EI@RF@RF_EI@P_E@RG_EI@P_E.T \\\n",
    "            + RF_EI@P@RG_EI@P_E.T \\\n",
    "            - RF_EI@P_E@RG_EI@RG@RG_EI@P_E.T \\\n",
    "            + RF_EI@P_E@RG_EI@P.T\n",
    "    \n",
    "    TSD = RF_EI@P_E@RG_EI@P_E.T\n",
    "\n",
    "    return -torch.trace(COST), -torch.trace(TSD)\n",
    "\n",
    "# for some reasons the adaptive filter is needed\n",
    "def adaptive_estimation(v_t, beta, square_term, i):\n",
    "    v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "    return v_t, (v_t/(1-beta**i))\n",
    "\n",
    "def MCA_LOSS_GIVEN_R(RP, track_cov, i, dim):\n",
    "    cov = RP + torch.eye((RP.shape[0])).cuda()*(.000001)\n",
    "    track_cov, cov_estimate = adaptive_estimation(track_cov, 0.5, cov, i)\n",
    "\n",
    "    cov_estimate_f = cov_estimate[:dim, :dim]\n",
    "    cov_f = cov[:dim, :dim]\n",
    "\n",
    "    cov_estimate_g = cov_estimate[dim:, dim:]\n",
    "    cov_g = cov[dim:, dim:]\n",
    "\n",
    "    LOSS = (torch.linalg.inv(cov_estimate)*cov).sum() - (torch.linalg.inv(cov_estimate_f)*cov_f).sum() -(torch.linalg.inv(cov_estimate_g)*cov_g).sum()\n",
    "    return track_cov, cov_estimate, LOSS\n",
    "\n",
    "# torch.cuda.set_device(3)\n",
    "\n",
    "save_path = './RESULTS_CROSS_SUBJECTS/'\n",
    "create_folder_if_not_exists(save_path)\n",
    "\n",
    "NET_1 = Advanced1DCNN_channel(SAMPLE_X.shape[1], 128, 4000).cuda()\n",
    "NET_2 = Advanced1DCNN_channel(SAMPLE_Y.shape[1], 128, 4000).cuda()\n",
    "\n",
    "classifier_major = ComplexClassifier(dim_features = 128, num_classes = 3).cuda()\n",
    "classifier_subtle = ComplexClassifier(dim_features = 128, num_classes = 11).cuda()\n",
    "classifier_subj = ComplexClassifier(dim_features = 128, num_classes = 25).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# lr1 = 0.0005\n",
    "# lr2 = 0.0005\n",
    "lr1 = 1e-3\n",
    "lr2 = 1e-3\n",
    "\n",
    "optimizer_1 = optim.Adam([\n",
    "    {'params': NET_1.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "optimizer_2 = optim.Adam([\n",
    "    {'params': NET_2.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "optimizer_classifier = optim.Adam([\n",
    "    {'params': classifier_major.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "    {'params': classifier_subtle.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "    {'params': classifier_subj.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "save_curve = []\n",
    "eig_list = []\n",
    "classifier_error = []\n",
    "\n",
    "test_primary_error_save = []\n",
    "test_error_save = []\n",
    "\n",
    "track_cov_final = torch.zeros((128+128)).cuda()\n",
    "track_cov_estimate_final = torch.zeros((128+128)).cuda()\n",
    "\n",
    "for i in range(1, 100000):\n",
    "    batch_size = 12\n",
    "    batch_indices = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    input_x = train_X[batch_indices]\n",
    "    input_y = train_Y[batch_indices]\n",
    "    \n",
    "    feature_1 = NET_1(input_x.cuda())\n",
    "    feature_2 = NET_2(input_y.cuda())\n",
    "    \n",
    "    RF = (feature_1.T@feature_1)/feature_1.shape[0]\n",
    "    RG = (feature_2.T@feature_2)/feature_2.shape[0]\n",
    "    P = (feature_1.T@feature_2)/feature_2.shape[0]\n",
    "\n",
    "    \n",
    "    input_dim, output_dim = RF.shape[1], RG.shape[1]\n",
    "    RFG = torch.zeros((input_dim+output_dim, input_dim+output_dim)).cuda()\n",
    "    RFG[:input_dim, :input_dim] = RF\n",
    "    RFG[input_dim:, input_dim:] = RG\n",
    "    RFG[:input_dim, input_dim:] = P\n",
    "    RFG[input_dim:, :input_dim] = P.T\n",
    "    \n",
    "    # track_cov_final, track_cov_estimate_final, COST = MCA_LOSS_GIVEN_R(RFG, track_cov_final, i, 128)\n",
    "    # COST, TSD = return_cost_trace(RFG, track_cov_estimate_final)\n",
    "    loss = fmcat_loss(RF, RF, P)\n",
    "    label_major_batch, label_subtle_batch, label_subj_batch = train_major_label[batch_indices], train_subtle_label[batch_indices], train_subj_label[batch_indices]\n",
    "\n",
    "    output_major, output_subtle, output_subj = classifier_major(feature_1.detach()), classifier_subtle(feature_1.detach()), classifier_subj(feature_1.detach())\n",
    "    # loss_1 = criterion(output_major, label_major_batch.cuda())\n",
    "    # loss_2 = criterion(output_subtle, label_subtle_batch.cuda())\n",
    "    # loss_3 = criterion(output_subj, label_subj_batch.cuda())\n",
    "    # (COST+loss_1+loss_2+loss_3).backward()\n",
    "    loss.backward()\n",
    "    # COST.backward()\n",
    "    # TSD.backward()\n",
    "    # print(\"====================================\")\n",
    "\n",
    "    # print(\"BEFORE UPDATE\")\n",
    "    # plist = list(NET_1.parameters())\n",
    "    # lp = sum(torch.norm(param.data) for param in plist) / len(plist)\n",
    "    # # print(f\"Param avg norm: {lp}\")\n",
    "    \n",
    "    # ps = list(NET_1.named_parameters())\n",
    "    # for name, param in ps[-2:-1]:\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name)\n",
    "    #         print(param.data)\n",
    "\n",
    "    # print(\"GRAD\")\n",
    "    # for name, param in ps[-2:-1]:\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name)\n",
    "    #         print(param.grad.data)\n",
    "\n",
    "    # torch.nn.utils.clip_grad_norm_(NET_1.parameters(), 1.0)\n",
    "    # torch.nn.utils.clip_grad_norm_(NET_2.parameters(), 1.0)\n",
    "    optimizer_1.step()  \n",
    "    optimizer_2.step()  \n",
    "    optimizer_classifier.step()  \n",
    "\n",
    "    # print(\"AFTER UPDATE\")\n",
    "    # plist = list(NET_1.parameters())\n",
    "    # lp = sum(torch.norm(param.data) for param in plist) / len(plist)\n",
    "    # print(f\"Param avg norm: {lp}\")\n",
    "    \n",
    "    # ps = list(NET_1.named_parameters())\n",
    "    # for name, param in ps[-2:-1]:\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name)\n",
    "    #         print(param.data)\n",
    "    # break\n",
    "    # print(\"====================================\")\n",
    "    # for param in NET_2.parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(param.grad.data)\n",
    "\n",
    "#     TSD = torch.logdet(track_cov_estimate_final) - torch.logdet(track_cov_estimate_final[:128, :128]) - torch.logdet(track_cov_estimate_final[128:, 128:])\n",
    "    # classifier_error.append([TSD.item(), loss_1.item(), loss_2.item(), loss_3.item()])\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_2.zero_grad()  \n",
    "    optimizer_classifier.zero_grad()\n",
    "\n",
    "        \n",
    "    if i%100 == 0:\n",
    "        # print('Iteration', i, 'Losses (Trace, Primary Movements, Sub Movements)', TSD.item(), loss_1.item(), loss_2.item())\n",
    "        # print(f\"Step {i}, TSD {TSD.item()}, COST {COST.item()}\")\n",
    "        print(f\"Step {i}, loss {loss.item()}\")\n",
    "        # plist = list(NET_1.parameters())\n",
    "        # lp = sum(torch.norm(param.data) for param in plist) / len(plist)\n",
    "        # print(f\"Param avg norm: {lp}\")\n",
    "                \n",
    "        # if i%100 == 0:\n",
    "\n",
    "            # np.save(save_path+'classifier_error_iter{0}.npy'.format(i), classifier_error)\n",
    "            # torch.save(NET_1.state_dict(), save_path+\"NET_1_iter{0}.pth\".format(i))\n",
    "            # torch.save(NET_2.state_dict(), save_path+\"NET_2_iter{0}.pth\".format(i))\n",
    "            # torch.save(classifier_major.state_dict(), save_path+\"classifier_major_iter{0}.pth\".format(i))\n",
    "            # torch.save(classifier_subtle.state_dict(), save_path+\"classifier_subtle_iter{0}.pth\".format(i))\n",
    "            # torch.save(classifier_subj.state_dict(), save_path+\"classifier_subj_iter{0}.pth\".format(i))\n",
    "\n",
    "            # NET_1.eval()\n",
    "            # classifier_major.eval()\n",
    "            # classifier_subtle.eval()\n",
    "            # classifier_subj.eval()\n",
    "\n",
    "            # class_major = []\n",
    "            # class_minor = []\n",
    "            # class_subj = []\n",
    "\n",
    "            # if i%100 == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         for k in range(0, 1000):\n",
    "\n",
    "            # #         for k in range(0, test_X.shape[0], 1):\n",
    "            #             output = NET_1(test_X[k:k+1].cuda())\n",
    "            #             output_major = classifier_major(output)\n",
    "            #             output_subtle = classifier_subtle(output)\n",
    "            #             output_subj = classifier_subj(output)\n",
    "\n",
    "            #             class_major.append(output_major.mean(0).detach().cpu().numpy())\n",
    "            #             class_minor.append(output_subtle.mean(0).detach().cpu().numpy())\n",
    "            #             class_subj.append(output_subj.mean(0).detach().cpu().numpy())\n",
    "\n",
    "            #         major_accuracy = np.mean(np.stack(class_major).argmax(1) == test_major_label[:np.stack(class_major).argmax(1).shape[0]].numpy())\n",
    "            #         subtle_accuracy = np.mean(np.stack(class_minor).argmax(1) == test_subtle_label[:np.stack(class_minor).argmax(1).shape[0]].numpy())\n",
    "                    \n",
    "            #         test_primary_error_save.append(major_accuracy.item())\n",
    "            #         test_error_save.append(subtle_accuracy.item())\n",
    "\n",
    "            #         print('Iteration', i, 'Accuracies (Primary Movements, Sub Movements)', np.array(test_primary_error_save).max(), np.array(test_error_save).max())\n",
    " \n",
    "            # NET_1.train()\n",
    "            # classifier_major.train()\n",
    "            # classifier_subtle.train()\n",
    "            # classifier_subj.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
